# SystÃ¨me RAG pour Documents Scientifiques (XML-TEI)

Un systÃ¨me **Retrieval Augmented Generation (RAG)** conÃ§u pour interroger des documents scientifiques historiques en **franÃ§ais**, au format **XML-TEI**, et gÃ©nÃ©rer des rÃ©ponses **sourcÃ©es** Ã  l'aide de modÃ¨les de langage.

âœ¨ **NouveautÃ©** : Support complet des modÃ¨les **Ollama** locaux, incluant **DeepSeek-R1** !

---

## FonctionnalitÃ©s clÃ©s

- Prise en charge des fichiers **XML-TEI**
- Recherche vectorielle avec **FAISS** (similaritÃ© cosinus ou **MMR** â€“ Maximal Marginal Relevance)
- **ModÃ¨les multiples** : cloud (HuggingFace, OpenRouter) + **local (Ollama)**
- **Support Ollama** : DeepSeek-R1, Llama, Mistral, et autres
- GÃ©nÃ©ration de rÃ©ponses **sourcÃ©es** avec citations automatiques
- Support **multilingue**, optimisÃ© pour le franÃ§ais
- Chargement rapide grÃ¢ce aux **embeddings prÃ©-calculÃ©s**
- Interface utilisateur **Streamlit** moderne et intuitive

---

## Modes de dÃ©ploiement

### 1. DÃ©mo en ligne (API uniquement)

AccÃ¨s direct sans installation : [**DÃ©mo Streamlit Cloud**](https://langchainragdemo-clqunhvtdazahepkgvopen.streamlit.app/)

### 2. ExÃ©cution locale avec Docker + Ollama

Support complet des modÃ¨les locaux et cloud.

---

## Installation

### Option 1 : DÃ©ploiement rapide avec Docker + Ollama

#### PrÃ©requis

- Docker et Docker Compose
- 8 GB de RAM minimum pour DeepSeek-R1

#### Ã‰tapes

```bash
git clone https://github.com/votre-repo/langchain_rag_demo.git
cd langchain_rag_demo

# Configuration Streamlit
mkdir -p .streamlit
echo "[server]
enableStaticServing = true" > .streamlit/config.toml

# ClÃ©s API
echo "[hf_api_key = \"YOUR_HF_API_KEY\" 
openrouter_api_key = \"YOUR_OPENROUTER_API_KEY\"]" > .streamlit/secrets.toml

# Lancer le script
chmod +x start-rag.sh
./start-rag.sh
```

Le script configure automatiquement :

- Le service Ollama avec DeepSeek-R1
- L'application Streamlit
- Le rÃ©seau Docker pour la communication entre services

**AccÃ¨s local** : http://localhost:8502

---

### Option 2 : Installation manuelle

#### 1. Installer Python 3.12

```bash
# Avec pyenv (recommandÃ©)
curl https://pyenv.run | bash
pyenv install 3.12.10
pyenv local 3.12.10
```

#### 2. Environnement virtuel

```bash
python -m venv .venv
source .venv/bin/activate   # Windows : .venv\Scripts\activate
```

#### 3. DÃ©pendances

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

#### 4. ClÃ©s API Streamlit

```bash
mkdir -p .streamlit
echo "[server]
enableStaticServing = true" > .streamlit/config.toml

echo "[hf_api_key = \"YOUR_HF_API_KEY\"
openrouter_api_key = \"YOUR_OPENROUTER_API_KEY\"]" > .streamlit/secrets.toml
```

#### 5. Lancement de l'application

```bash
streamlit run app.py
```

---

## Architecture du pipeline

### 1. Traitement des documents

- **Sources** : `./data/`, fichiers uploadÃ©s ou corpus prÃ©dÃ©fini
- **Parsing XML-TEI** : extraction du titre, date, auteurs et contenu
- **Fragmentation** : `RecursiveCharacterTextSplitter` (2500 caractÃ¨res, chevauchement de 800)

### 2. GÃ©nÃ©ration des embeddings

#### Mode rapide

Utiliser le script `generate_embeddings_pre_computed.py` pour crÃ©er des embeddings avec [intfloat/multilingual-e5-large-instruct](https://huggingface.co/intfloat/multilingual-e5-large-instruct), stockÃ©s dans `/embeddings/`.

1. Cocher **Utiliser les embeddings prÃ©-calculÃ©s**
2. Cliquer sur **Charger les embeddings prÃ©-calculÃ©s**

#### Mode personnalisÃ©

Utilise [sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2).

**Deux modes :**

1. **Traitement du corpus complet** dans `/data/`
   - Cliquer sur **Traiter le corpus par dÃ©faut**

2. **Traitement de fichiers uploadÃ©s uniquement**
   - Cocher **Utiliser uniquement les fichiers tÃ©lÃ©chargÃ©s**
   - SÃ©lectionner les fichiers via **Browse files**
   - Cliquer sur **Traiter les documents**

### 3. Recherche vectorielle

**StratÃ©gies disponibles :**
- **Cosine Similarity** : prÃ©cision maximale
- **MMR** : Ã©quilibre pertinence/diversitÃ©

### 4. ModÃ¨les de langage

#### Locaux (Ollama)

| ModÃ¨le | Taille | Performance | Cas d'usage |
|--------|--------|-------------|-------------|
| **DeepSeek-R1** | 1.5Bâ€“7B | â­â­â­â­â­ | Raisonnement avancÃ©, analyse fine |
| Llama 3.2 | 3Bâ€“70B | â­â­â­â­ | Usage gÃ©nÃ©ral, conversations |
| Mistral | 7B | â­â­â­ | Rapide, efficace |

#### Cloud (API HuggingFace / OpenRouter)

| Service | ModÃ¨le | Avantages |
|---------|--------|-----------|
| **OpenRouter** | Llama 4 Maverick | DerniÃ¨re gÃ©nÃ©ration, gratuit |
| | Gemma-3n-e4b | Contexte Ã©tendu (32K), multilingue |
| | Qwen3-32B | Logique avancÃ©e, 131K tokens |
| **HuggingFace** | Zephyr-7B | Bonne prÃ©cision factuelle |
| | Mistral-7B | SpÃ©cialisÃ© science et extraction d'infos |

---

## Structure du projet

```
langchain_rag_demo/
â”œâ”€â”€ .streamlit/
â”‚   â”œâ”€â”€ config.toml              # Config serveur Streamlit
â”‚   â””â”€â”€ secrets.toml             # ClÃ©s API
â”œâ”€â”€ data/                         # Documents XML-TEI
â”œâ”€â”€ embeddings/                   # Index FAISS + mÃ©tadonnÃ©es
â”œâ”€â”€ static/                       # Fichiers statiques (logo, etc.)
â”œâ”€â”€ tmp/                          # Fichiers temporaires
â”œâ”€â”€ vector_store/                 # Stockage alternatif FAISS
â”œâ”€â”€ app.py                        # Application principale
â”œâ”€â”€ ollama_utils.py               # Utilitaires Ollama
â”œâ”€â”€ generate_embeddings_pre_computed.py
â”œâ”€â”€ requirements.txt              # DÃ©pendances Python
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ start-rag.sh                  # Script de dÃ©marrage
â””â”€â”€ README.md
```

---

## Configuration XML-TEI

### Balises requises

```xml
<TEI xmlns="http://www.tei-c.org/ns/1.0">
  <teiHeader>
    <titleStmt>
      <title>Titre du document</title>
    </titleStmt>
    <sourceDesc>
      <p><date when="1995">1995</date></p>
    </sourceDesc>
  </teiHeader>
  <text>
    <body>
      <p>Contenu principalâ€¦</p>
    </body>
  </text>
</TEI>
```

### MÃ©tadonnÃ©es extraites

- **Titre** : `<tei:title>`
- **Date** : `<tei:date>` (annÃ©e extraite automatiquement)
- **Contenu** : `<tei:p>`

---

## Prompt Engineering

### Framework COSTAR intÃ©grÃ©

Optimisation structurÃ©e des requÃªtes et des rÃ©ponses :

- **C**ontexte : Documents scientifiques XML-TEI
- **O**bjectif : RÃ©ponses factuelles basÃ©es sur les sources
- **S**tyle : Markdown structurÃ© avec citations
- **T**on : AcadÃ©mique, formel et prÃ©cis
- **A**udience : Chercheurs, historienÂ·nes
- **R**Ã©ponse : Citations obligatoires, niveau de confiance

---

## Contribution

1. Fork du projet
2. CrÃ©ation d'une branche :
   ```bash
   git checkout -b feature/nouvelle-fonctionnalite
   ```
3. Commit :
   ```bash
   git commit -m 'Ajout nouvelle fonctionnalitÃ©'
   ```
4. Push :
   ```bash
   git push origin feature/nouvelle-fonctionnalite
   ```
5. Ouvrir une Pull Request

---

## Licence

DistribuÃ© sous licence **MIT**.

---

## Auteur & Ã‰quipe

**DÃ©veloppÃ© par** [Mikhail Biriuchinskii](https://www.linkedin.com/in/mikhail-biriuchinskii/)  
IngÃ©nieur TAL â€¢ Ã‰quipe **ObTIC**, Sorbonne UniversitÃ©

ğŸ”— https://obtic.sorbonne-universite.fr/

â­ **Si ce projet vous est utile, pensez Ã  lui attribuer une Ã©toile !**